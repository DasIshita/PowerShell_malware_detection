from keras import models, layers, optimizers
from xgboost import XGBClassifier
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, recall_score, confusion_matrix
import DataLoader

malicious_directory = '../mspd/malicious_pure'
benign_directory = '../mspd/powershell_benign_dataset'
mixed_malicious_directory = '../mspd/mixed_malicious'

#CNN
baseline_CNN = models.Sequential()
baseline_CNN.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu'))
baseline_CNN.add(layers.Conv2D(64, (3, 3), activation='relu'))
baseline_CNN.add(layers.MaxPooling2D(pool_size=(2, 2)))
baseline_CNN.add(layers.Dense(2, activation='softmax'))
opt = optimizers.Adam(learning_rate=0.001)
baseline_CNN.compile(loss='categorical_crossentropy', optimizer=opt)

#MLP
baseline_MLP = models.Sequential()
baseline_MLP.add(layers.Dense(400, activation='relu'))
baseline_MLP.add(layers.Dense(2, activation='softmax'))
baseline_MLP.compile(loss='categorical_crossentropy', optimizer=opt)

#XGBoost
baseline_XGB = XGBClassifier()

#SVM
baseline_SVM = svm.SVC(kernel='poly', gamma=0.1)

#Random Forest
baseline_RF = RandomForestClassifier(n_estimators=100)

#summarize all base models
baseModels=[baseline_RF, baseline_CNN, baseline_MLP, baseline_XGB, baseline_SVM]


#generate datasets
X_train, X_test, y_train, y_test = DataLoader.generateData(benign_directory, malicious_directory, mixed_malicious_directory)
print(y_test)


for model in baseModels[0]:
    model.fit(X_train, Y_train, verbose=1) #, epoches=30, batch_size=200, 
    y_pred = model.predict(X_test)
    print("Accuracy:", accuracy_score(y_pred, Y_test))
    print("MSE:", mean_squared_error(y_pred, Y_test))
    print("Precision:", precision_score(y_pred, Y_test, average='macro'))
    print("Recall:", recall_score(y_pred, Y_test, average='macro'))


