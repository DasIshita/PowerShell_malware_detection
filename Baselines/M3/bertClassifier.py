from keras import models, layers, optimizers
from torch.utils.data import DataLoader
from tqdm import tqdm
import re
import os
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score
from sklearn.metrics import roc_auc_score
import torch
import matplotlib.pyplot as plt

from transformers import BertTokenizer, BertModel
from transformers import BertTokenizerFast, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from keras import models, layers, optimizers
import torch
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from transformers import create_optimizer
import tensorflow as tf
import pyarrow as pa
import pyarrow.dataset as ds
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import DataCollatorWithPadding
from transformers import TFAutoModelForSequenceClassification
import torch.nn.functional as F
import torch

torch.cuda.empty_cache()

malicious_directory = '../../mspd/malicious_pure'
benign_directory = '../../mspd/powershell_benign_dataset'
mixed_malicious_directory = '../../mspd/mixed_malicious'
obfuscated_mixed_directory = '../../mspd/obfuscated_mixed'
string_mixed_directory = '../../mspd/STRING_obfuscated_mixed'


def preprocessing(directory):
    count=100
    
    # tz = BertTokenizer.from_pretrained("bert-base-uncased")
    scripts = []
    for filename in tqdm(list(os.scandir(directory))):
        # if count == 0:
        #     break

        if filename.is_file():
            with open(filename, encoding="ISO-8859-1") as f:
                lines = f.readlines()

            f.close()

            document = []
            
            if len(lines) == 0: #some obfuscated files are empty.
                continue

            for sen in range(len(lines)):

                # Remove all the special characters
                sent = re.sub(r'\W', ' ', str(lines[sen]))
                
                #sent = str(lines[sen])
                # Remove single characters from the start
                sent = re.sub(r'\[^a-zA-Z*"$"]\s+', ' ', sent) 

                # Substituting multiple spaces with single space
                sent = re.sub(r'\s+', ' ', sent, flags=re.I)

                # Converting to Lowercase
                sent = sent.lower()
                
                if len(sent.strip()) > 0:
                    document.extend(sent.split())
  
                 #document.append(sent)
#                 newdoc = []
#                 for i in range(len(document)):
#                     if document[i] != ' ':
#                         newdoc += document[i].split()
            count -= 1
            newdoc = ' '.join(document)
            
            # #Use Bert tokenizer to get the input_ids, input_masks, and input_type_ids
            # encoded = tz.encode_plus(
            # text=newdoc,  # the sentence to be encoded
            # add_special_tokens=True,  # Add [CLS] and [SEP]
            # max_length = 512,
            # # truncation=True,
            # pad_to_max_length=True,  # Add [PAD]s
            # return_attention_mask = True,  # Generate the attention mask  # ask the function to return PyTorch tensors
            # )
            # input_ids = encoded['input_ids']
            # attn_mask = encoded['attention_mask']
            # input_type_ids = encoded["token_type_ids"]
                      
            # scripts.append(torch.tensor(input_ids+attn_mask+input_type_ids))
            scripts.append(newdoc)
            
    return scripts

def preprocess_function(examples):
    
    return tokenizer(examples["text"], truncation=True)






bscripts = preprocessing(benign_directory)
mscripts = preprocessing(malicious_directory)
Mixed = preprocessing(mixed_malicious_directory)
Obfus = preprocessing(obfuscated_mixed_directory)
Strings = preprocessing(string_mixed_directory)

X = bscripts+mscripts
y = [0]*len(bscripts) + [1]*len(mscripts)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
Mix_y = [1]*len(Mixed) + y_test
Obs_y = [1]*len(Obfus) + y_test
str_y = [1]*len(Strings) + y_test
Mixed += X_test
Obfus += X_test
Strings += X_test

#Convert to the Hugging Face dataset

df = pd.DataFrame({'text': X_train, 'label': y_train})
train_dataset = Dataset(pa.Table.from_pandas(df))
df = pd.DataFrame({'text': X_test, 'label': y_test})
test_dataset = Dataset(pa.Table.from_pandas(df))
df = pd.DataFrame({'text': Mixed, 'label': Mix_y})
mixed_dataset = Dataset(pa.Table.from_pandas(df))
df = pd.DataFrame({'text': Obfus, 'label': Obs_y})
TOKEN_dataset = Dataset(pa.Table.from_pandas(df))
df = pd.DataFrame({'text': Strings, 'label': str_y})
STRING_dataset = Dataset(pa.Table.from_pandas(df))

mspd = DatasetDict({"train":train_dataset,"test":test_dataset,"mixed":mixed_dataset,"TOKEN":TOKEN_dataset, "STRING":STRING_dataset})



tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_mspd = mspd.map(preprocess_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
print("Finish Tokenization!!!!!!!!!!!!")





# model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
# training_args = TrainingArguments(
#     output_dir="./results",
#     learning_rate=2e-5,
#     per_device_train_batch_size=16,
#     per_device_eval_batch_size=16,
#     num_train_epochs=5,
#     weight_decay=0.01,)
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=tokenized_mspd["train"],
#     eval_dataset=tokenized_mspd["test"],
#     tokenizer=tokenizer,
#     data_collator=data_collator,)
# trainer.train()


tf_train_set = tokenized_mspd["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)
tf_validation_set = tokenized_mspd["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
tf_mixed_set = tokenized_mspd["mixed"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
tf_TOKEN_set = tokenized_mspd["TOKEN"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
tf_STRING_set = tokenized_mspd["STRING"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)
batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_mspd["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)


model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
model.compile(optimizer=optimizer) #, loss='binary_crossentropy'
model.fit(tf_train_set, epochs=3)

y_preds = model.predict(tf_validation_set)
y_pred = np.argmax(y_preds.logits, axis=1)

Mix_preds = model.predict(tf_mixed_set)
Mix_pred = np.argmax(Mix_preds.logits, axis=1)

Obs_preds = model.predict(tf_TOKEN_set)
Obs_pred = np.argmax(Obs_preds.logits, axis=1)

str_preds = model.predict(tf_STRING_set)
str_pred = np.argmax(str_preds.logits, axis=1)


print("########## Test 1 ###########")
print(accuracy_score(y_test, y_pred))
print(recall_score(y_test, y_pred, average="macro"))
print(precision_score(y_test, y_pred, average="macro"))
print(f1_score(y_test, y_pred, average="macro"))
print(roc_auc_score(y_test, y_pred))


print("########## Test 2 ###########")
print(accuracy_score(Mix_y, Mix_pred))
print(recall_score(Mix_y, Mix_pred, average="macro"))
print(precision_score(Mix_y, Mix_pred, average="macro"))
print(f1_score(Mix_y, Mix_pred, average="macro"))
print(roc_auc_score(Mix_y, Mix_pred))

print("########### TEST 3 ##############")
print(accuracy_score(Obs_y, Obs_pred))
print(recall_score(Obs_y, Obs_pred, average="macro"))
print(precision_score(Obs_y, Obs_pred, average="macro"))
print(f1_score(Obs_y, Obs_pred, average="macro"))
print(roc_auc_score(Obs_y, Obs_pred))

print("########### TEST 4 ##############")
print(accuracy_score(str_y, str_pred))
print(recall_score(str_y, str_pred, average="macro"))
print(precision_score(str_y, str_pred, average="macro"))
print(f1_score(str_y, str_pred, average="macro"))
print(roc_auc_score(str_y, str_pred))














'''
Pytorch Bert Model
'''
# model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')
# optimizer = torch.optim.Adam(model.parameters(), lr=0.005, betas=(0.9,0.999))
# for epoch in range(1):  # loop over the dataset multiple times

#     running_loss = 0.0
#     for i in range(len(X_train)):
#         # get the inputs; data is a list of [inputs, labels]
#         inputs = X_train[i]
#         labels = y_train[i]

#         # zero the parameter gradients
#         optimizer.zero_grad()

#         # forward + backward + optimize
#         outputs = model(inputs)
#         loss = criterion(outputs, labels)
#         loss.backward()
#         optimizer.step()

#         # print statistics
#         running_loss += loss.item()
#         if i % 2000 == 1999:    # print every 2000 mini-batches
#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
#             running_loss = 0.0

# print('Finished Training')
# print('Start Testing')
# for i in range(len(X_test)):
#     # get the inputs; data is a list of [inputs, labels]
#     inputs = X_test[i]
#     y_pred = model(inputs)
#     print(y_pred)


'''
Transformers Bert Model
'''

# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# Bmodel = BertModel.from_pretrained("bert-base-uncased")

# for ex in X_test:
#     encoded_input = tokenizer(ex[:512], return_tensors='pt')
#     output = Bmodel(**encoded_input)
#     #add softmax layer
#     print(output)
